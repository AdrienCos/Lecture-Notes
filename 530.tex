\documentclass[10pt,letterpaper,landscape]{report}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{enumitem}
\setlist{nosep}


\usepackage{graphicx}

\usepackage[left=-0.5cm,right=-0.5cm,top=-0.3cm,bottom=-0.3cm]{geometry}


\author{Adrien Cosson}
\title{Fiches de r√©vision}

\newcommand{\boxheight}{21.59cm}
\newcommand{\boxwidth}{8.85cm}


\begin{document}
\begin{small}
\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}

    \textbf{Power consumption issue}\\
    Power used by a chip is $P \approx \frac{1}{2}CV_{dd}^2Af$ \\
    Naive solution: reduce $V_{dd}$
    \begin{itemize}
        \item Leakage limits how low you can go
        \item Other solution: reduce $f$ and increase nb of cores\\
        $\Rightarrow$ needs programs to be parallelizable
    \end{itemize}
    
    \textbf{Five principles of perf.}
    \begin{itemize}
        \item Parallelism
        \item Speculation
        \item Locality
        \item Memorization
        \item Amdahl's Law
    \end{itemize}{}

    \textbf{Parallelism}\\
    $T_n$: time to compute a problem with $n$ cores \\
    Average parallelism : $P_{avg} = \frac{T_1}{T_\infty}$\\
    For a $p$ wide system:\\
    $T_p \geq \max(\frac{T_1}{p}, T_\infty)$ \\
    $P_{avg} \gg p \Rightarrow T_p \approx \frac{T_1}{p}$

    \textbf{Speculation}\\
    Make educated guesses to avoid expensive operations, if you can be right most of the time
    
    \textbf{Locality}
    \begin{itemize}
        \item Temporal: if you accessed data, you are likely to access it again soon
        \item Spatial: if you accessed data, you are likely to access its neighbour soon
    \end{itemize}
    
    \textbf{Memorization}\\
    Remember the answer to expensive computations in case it comes up again
    
    \textbf{Amdahl's Law}\\
    If we can increase a fraction $f$ of a task by a factor of $S_{task}$:\\
    $\boxed{S_{total} = \dfrac{1}{(1-f) + \dfrac{f}{S_{task}}}}$
    
    \textbf{Performace measurement}\\
    \textit{Latency}: time to perform a fixed task\\
    \textit{Throughput}: number of tasks in a fixed time\\
    $\Rightarrow$ not most contradictory
    
    \textbf{Perf. improvement}\\
    Proc. A is $X$ times faster than proc. B if:
    \begin{itemize}
        \item $Lat(A) = Lat(B) / X$
        \item $Thr(A) = Thr(B) \times X$
    \end{itemize}
    Proc. A is $X\%$ faster than proc. B if:
    \begin{itemize}
        \item $Lat(A) = Lat(B) / (1 + X/100)$
        \item $Thr(A) = Thr(B) * (1 + X/100)$
    \end{itemize}
    
    \textbf{Averaging perf.}\\
    Latencies can be added, but not throughputs:\\
    $Lat(P1 + P2) = Lat(P1) + Lat(P2)$\\
    $Thr(P1 + P2) = \frac{1}{\frac{1}{Thr(P1)} + \frac{1}{Thr(P2)}}$
    
    \textbf{Averages}\\
    Arithmetic (for time): $\frac{1}{N}\sum_i f(i)$\\
    Harmonic (for rates): $\frac{N}{\sum_i \frac{1}{f(i)}}$\\
    Geometric (for ratio): $\sqrt[N]{\prod_i f(i)}$
    
    

\end{minipage}
}\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}

    \textbf{Processor perf.}\\
    $Perf = \underbrace{\frac{instructions}{program}}_{executed\ instructions} \times \underbrace{\frac{cycles}{instruction}}_{CPI} \times \underbrace{\frac{time}{cycle}}_{clock}$

    \textbf{MIPS}\\
    $MIPS= CPI \times clock \times 10^{-6}$
    
    \textbf{Power-perf. metrics}\\
    Power-delay: $PDP = P_{avg} \times t$\\
    Energy-delay: $EDP = PDP \times t$\\
    Energy-delay squared: $EDDP = EDP * t$
    
    \textbf{ISA}\\
    Instruction Set Architecture: abstraction layer between the software and hardware (facilitates dev., provides portability) 
    
    \textbf{Good ISA traits}
    \begin{itemize}
        \item Programmability: easy to express programs efficiently
        \item Implementability: easy to make high-perf / low-power / low-cost / etc. implementations
        \item Compatibility: new generations do not break old software
    \end{itemize}
    
    \textbf{Sequential Model}
    \begin{enumerate}
        \item Fetch
        \item Decode
        \item Read
        \item Execute
        \item Write
    \end{enumerate}
    
    \textbf{Seven dimensions of ISA}
    \begin{enumerate}
        \item Class: load-store, register-memory,\dots
        \item Instr. encoding: fixed or variable length
        \item Types \& sizes of operands: 8-bit, 16-bit,\dots
        \item Addressing mode: register, immediate, \dots
        \item Memory addressing: byte-aligned, not aligned
        \item Operations: arithmetic, control, \dots
        \item Control flow: conditional branches, jump, \dots
    \end{enumerate}
    
    \textbf{Class: operand model}\\
    How many explicit operands?
    \begin{itemize}
        \item 3: general purpose
        \item 2: multiple explicit accumulators
        \item 1: one implicit accumulator
        \item 0: hardware stack
        \item 4+: special cases
    \end{itemize}
    
    \textbf{Instruction length}
    \begin{itemize}
        \item Fixed length: simple implementation, low code density
        \item Variable length: high code density, complex fetch
    \end{itemize}
    de
    \textbf{Addressing mode}
    \begin{itemize}
        \item Memory: fundamental storage space
        \item Registers: faster than memory, small number, directly addressed
        \item Immediate: part of the instruction, input only
    \end{itemize}
    
    \textbf{RISC vs CISC}\\
    CISC: higher density, less inst/program, more cycles/inst and seconds/cycle when compared to RISC
    
    \textbf{Pipelining}\\
    Work on multiple instructions at the same time\\
    $\iff$ increase throughput at the expense of latency
    
    


\end{minipage}
}\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}


    \textbf{Fetch}
    \begin{itemize}
        \item Use PC to index memory
        \item Increment PC
        \item Store the instruction in the IR
    \end{itemize}
    
    \textbf{Decode}
    \begin{itemize}
        \item Decode opcode bits
        \item Read input operands from register file
        \item Write state to pipeline register (opcode, register contents, offset/destination fields, PC+1)
    \end{itemize}
    
    \textbf{Execute}
    \begin{itemize}
        \item Perform ALU operations
        \item Write state to pipeline register (ALU results, PC+1+offset, instruction bits for opcode, destination registers)
    \end{itemize}
    
    \textbf{Load/Store}
    \begin{itemize}
        \item Perform data cache access for memory operations
        \item Write state to the pipeline register (ALU results, memory data address, instruction bits for opcode, destination registers)
    \end{itemize}
    
    \textbf{Write back}
    \begin{itemize}
        \item Write back result to register file (if needed)
        \item 
    \end{itemize}
    
    \textbf{Pipeline cycle time}\\
    $T_{cyc}  = \dfrac{sum(T_{cyc, i})_i}{max(T_{cyc, i})_i}$

    \textbf{Dependencies}
    \begin{itemize}
        \item RAW (true dependency)
        \item WAR (anti-dependency)
        \item WAW (output dependency)
    \end{itemize}
    
    \textbf{Hazards terminology}
    \begin{itemize}
        \item Pipeline hazards: potential violation of program dependencies
        \item Hazard resolution: static or dynamic
        \item Pipeline interlock: hardware mechanism for dynamic hazard resolution
    \end{itemize}
    
    \textbf{Data hazard avoidance}
    \begin{itemize}
        \item Avoidance (static, use \texttt{noop})
        \item Detect and stall (stall until later inst. finishes)
        \item Detect and forward (get correct value from somewhere in the pipeline)
    \end{itemize}
    
    \textbf{Avoidance issue}
    \begin{itemize}
        \item Different hardware may require different \texttt{noop} arrangement
        \item Longer code (more cache footprint, longer binary load time, terrible in machine that have more than 1 inst./sec)
    \end{itemize}
    
    \textbf{Detect and stall}
    \begin{itemize}
        \item Detect: compare regA and regB with DestReg of previous inst.
        \item Stall: pass \texttt{noop} to Execute
    \end{itemize}
    
    \textbf{Detect and Forward}
    \begin{itemize}
        \item Detect: as before
        \item Forward: propagate the instruction and flag it, use the flag to select a datapath that send the previous output of the ALU back in its input
    \end{itemize}

\end{minipage}
}

\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}

    \textbf{Control hazard avoidance}
    \begin{itemize}
        \item Avoidance (convert branches to predication)
        \item Detect and stall (stop fetching until branch resolution)
        \item Speculate and squash (keep going beyond branch, discard if wrong)
    \end{itemize}
    
    \textbf{Branch delay slot}\\
    Instruction executed between a branch and a target instructions, in order to not waste a squashed cycle in the event of a wrong prediction. This instruction should be one that will always be executed, no matter the result of the branch.
    
    \textbf{Branch prediction possibilities}
    \begin{itemize}
        \item Biased for not taken
        \item Software prediction (extra bit in the instruction)
        \item Branch offset (static, look at offset: positive = not taken, negative = taken)
        \item Branch history (dynamic, done in HW)
    \end{itemize}
    
    \textbf{Dynamic branch prediction}
    \begin{enumerate}
        \item Target predictor (for control branches, easy)
        \item Direction predictor (for conditional branches, hard)
    \end{enumerate}
    
    \textbf{Branch target buffer (BTB)}\\
    Small cache (address = PC, data = target), if hit then known branch, if miss then not branch or unknown branch. Partial address/data, false positives will be detected later in the pipeline.\\
    $\Rightarrow$ direct targets work well, indirect is more mixed (OK for indirect calls)
    
    \textbf{Return address stack (RAS)}\\
    Stack on which all return addresses are stored, and can be used as a BTB alternative for indirect returns
    
    \textbf{TODO: take notes on missed lecture (Sept 19)}
    
    \textbf{Branch prediction acronyms}\\
    \textit{BHT}: Branch History Table\\
    \textit{PHT}: Pattern History Table
    
    \textbf{GShare predictor}\\
    Two-level branch predictor, where the global BHT is XOR'd with the branch PC to access the PHT
    
    \textbf{Hybrid predictor}\\
    Implements multiple branch predictors, and predicts which one to chose with a meta-predictor
    
    \textbf{Basic Cache}\\
    On memory read:
    \begin{itemize}
        \item Hit: return data
        \item Miss: choose and displace used block, fetch from lower memory, store and return data
    \end{itemize}
    
    \textbf{Cache terminology}
    \begin{itemize}
        \item Frame: location in the cache
        \item Block/cache line: minimum data unit
        \item Hit: block found
        \item Miss: item not found
        \item Miss ratio: fraction of calls that miss
        \item Hit time: time to access the cache
        \item Miss penalty: time to replace block and return it to upper level
    \end{itemize}
    
\end{minipage}
}\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}

    \textbf{TODO: catch up with caching lecture}
    
    \textbf{Write policies}
    \begin{itemize}
        \item On hit, update memory?
        \begin{itemize}
            \item Yes: write-through
            \item No: write-back
        \end{itemize}
        \item On miss, allocate a cache block frame?
        \begin{itemize}
            \item Yes: write-allocate
            \item No: no-write-allocate
        \end{itemize}
    \end{itemize}
    
    \textbf{Write buffer}\\
    When writing, store in a buffer (between CPU and L1), and only stall when the buffer is full. A problem can appear in the case of data dependency.
    
    \textbf{Writeback buffer}\\
    Like a write cache, but between other cache levels (L1-L2 for instance)
    
    \textbf{Miss classification}
    \begin{itemize}
        \item Compulsory (cold miss)
        \item Capacity (not enough space)
        \item Conflict (restrictive mapping strat.)
        \item Coherence (multiprocessor causing miss)
    \end{itemize}
    
    \textbf{Victim cache}\\
    Fully associative cache placed on the refill path of a directly mapped cache, made to receive evicted lines.
    
    \textbf{Reducing miss cost}
    \begin{itemize}
        \item Refill the cache with the critical word first, and restart cache reading before the refill is complete 
        \item Split the cache blocks into sub-blocks, with a separate valid bit per sub-block. Only bring required sub-blocks into the cache
    \end{itemize}
    
    \textbf{Multi-level cache access time}\\
    $t_{avg} = t_{hit} + p_{miss} \times t_{miss} $ 
    
\end{minipage}
}\fbox{
\begin{minipage}[t][\boxheight][c]{\boxwidth}


	TODO



\end{minipage}
}
\end{small}
\end{document}
